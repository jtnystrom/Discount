3.0.1

  Bugfix for self-union or self-intersection of indexes (caused 
a cartesian product).
  Additional convenience methods for Index, e.g. unionLeft, unionRight, etc.

3.0.0

  Support for indexes (k-mer databases) written as parquet files. 
  Index operations such as union, intersect, subtract, with various combination
rules like min, max, sum, left, right.
  Restructured the API to use indexes as much as possible.
  Several operations were moved to Spark SQL (from handcrafted Scala) 
for performance and simplicity.
  Run scripts were renamed and can now detect their location, which makes it 
easy to symlink them to somewhere in $PATH.
  The new -p flag is now the preferred way to specify the number of partitions.
  Most commands that take input can now read input from an index (using -i) 
as well as from sequence files.
  K-mer counts are now consistently represented as Int instead of Long in the 
user API as they were limited to 32-bit signed integers internally.
  Added com.globalmentor's hadoop-bare-naked-local-fs to avoid dependency on 
winutils.exe on Windows when running tests.
  Various simplifications and speedups.

2.3.0

  Pre-grouped mode for handling repetitive or very large data, which can be 
enabled with --method pregrouped. This greatly increases the maximum data size 
that can be analysed.
  Some minimizer sets are now bundled in the Discount jar, which means that 
many users will not need to supply minimizers manually.
  Improved support for large m (up to 13), which helps subdivide complex data 
with many distinct k-mers.
  Automatic coalescing of partitions in frequency sampling when appropriate 
(improves performance).
  Support for @inputs.txt syntax to supply a list of input files on the command 
line.
  More efficient frequency sampling by doing the sampling entirely in Spark 
SQL, instead of partially on the driver.

2.2.1

  This release fixes a bug in fastq parsing, where sometimes data would be 
dropped.

2.2.0

  Improved support for very long fasta sequences (e.g. full chromosomes), even 
for multiple sequences per file. This is done by relying on an external .fai 
index, which is now necessary for sequences with unbounded length.
  File input formats can now be mixed (e.g. fastq, fasta, long fasta can be 
read by the same job).
  k-mer statistics can now optionally be written to an output file using a new 
argument (not just to standard output as before).
  For convenience, additional PASHA minimizer sets for k >= 19, m=10,11 were 
added to the distribution.

2.1.0

  Classes were restructured under the com.jnpersson.discount package (instead 
of simply "discount") to comply with normal Java/Scala conventions. This is a 
breaking change for API users, but should be a simple migration.
  Faster algorithms for read splitting and bitwise encoding.
  Sampling and input parsing has changed into a unified API that is consistent 
across short reads and long sequences, and that samples long sequences more 
fairly.
  Foundational work towards preserving the sequence locations of input sequence 
fragments.
  Additional test cases for different kinds of input data.

2.0.1

  This release fixes a bug where long, multiline input sequences were not 
handled correctly and k-mer counts would occasionally be wrong, along with some 
other minor improvements.

2.0.0

  Nearly 50% faster counting due to better algorithms, including a version of 
radix sort from the Fastutil library
  Automatic selection of the most appropriate minimizer set from a directory, 
by matching with the desired (k, m) values
  Support for interactive notebooks (a Zeppelin example is included) and a 
restructured API to support this
  Hashed superkmers can now be queried by sequences to find matching k-mers
  Support for lowercase nucleotide letters in input
  Support for user-defined minimizer orderings (-o given)
  Various simplifications and enhancements

1.4.0

  Scala 2.12/Spark 3.1 are now the default versions when compiling.
  Bugfix for incorrect counting when k mod 16 = 0.
  sbt-assembly is now the preferred way to package Discount, including its 
dependencies (Scallop and Fastdoop) in a "fat" jar.
  Additional property-based unit tests using ScalaCheck.
  A minimal demo application (ReadSplitDemo) shows how to use the Discount API 
without Spark.
  Various simplifications, code cleanups and speedups.

1.3.0

  Improved performance for large m
  Reduced memory usage in the hashing stage
  Fixed a bug that caused Discount to crash on empty inputs
  Improved command line argument validation
  Renamed the output path for count --stats
  Renamed the command line arguments --motif-set and --stats to --minimizers 
and --buckets, respectively, for improved clarity

1.2.0

  Includes PASHA sets for k = 28,55 instead of DOCKS sets for k = 20,50
  Support for random minimizer orderings
  Human-readable minimizer output in per-bucket stats for minimizer analysis
  Additional unit tests
  Bugfixes for motifs at the very start of a k-length window, which were not 
properly detected during hashing
  Bugfix for handling of EOF in Fastdoop

1.1.0

  FASTA output by default when writing a counts table (--tsv can be used to get 
a simple tsv table)
  Normalization of k-mer orientation (forward and reverse complement treated as 
the same value). This is a little slower than the non-normalized mode, however.
  Configurable input split sizes in the run scripts (instead of hardcoded as 
before)
  A run script for AWS EMR (experimental)
  Improved command line help and validation of parameters

1.0.0 (Spark 2.4)

  Initial release, compiled with Spark 2.4.6 libraries.
