#!/bin/bash
#Copy this file to spark-submit.sh and edit the config variables.

#Run everything in one process (don't forget to adjust Spark's driver memory)
MASTER=local[*]

#Full cluster running independently
#MASTER=spark://localhost:7077

#To reduce memory usage for big inputs, increase PARTITIONS. Default is 200 if unset.
#PARTITIONS="spark.sql.shuffle.partitions=4000"

#Set this variable to the location of your Spark distribution.
SPARK=/path/to/spark-x.x.x-hadoop

#For standalone mode (one process), it is helpful to provide as much memory as possible.
MEMORY=spark.driver.memory=16g

#Scratch space location. This has a big effect on performance; should ideally be a fast SSD or similar.
LOCAL_DIR="spark.local.dir=/tmp"

#Max size of input splits in bytes. A smaller number reduces memory usage but increases the number of 
#partitions for the first stage. If this variable is unset, Spark's default of 128 MB will be used.
#SPLIT="spark.hadoop.mapreduce.input.fileinputformat.split.maxsize=$((64 * 1024 * 1024))"

#If using Scala 2.11: Change scala-2.12 to scala-2.11 below.
#On Windows: Change bin/spark-submit to bin/spark-submit.cmd.

#--conf $SPLIT
#--conf $PARTITIONS
exec $SPARK/bin/spark-submit \
  --conf spark.driver.maxResultSize=2g \
  --conf $MEMORY \
  --conf $LOCAL_DIR \
  --master $MASTER \
  --class com.jnpersson.discount.spark.Discount target/scala-2.12/Discount-assembly-2.2.1.jar $*
