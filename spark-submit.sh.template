#!/bin/bash
#Copy this file to spark-submit.sh and edit the config variables.

MASTER=local[*]
#If you are running a standalone cluster, use the following instead
#MASTER=spark://localhost:7077

SPARK=/path/to/spark-2.4.X-bin-hadoopX.X

#To reduce memory usage for big inputs, increase PARTITIONS
#PARTITIONS="spark.sql.shuffle.partitions=4000"
PARTITIONS="spark.sql.shuffle.partitions=400"

#Max size of input splits in bytes. A smaller number reduces memory usage but increases the number of 
#partitions for the first stage. If this variable is unset, Spark's default of 128 MB will be used.
SPLIT="spark.hadoop.mapreduce.input.fileinputformat.split.maxsize=$((64 * 1024 * 1024))"

#If using Scala 2.11: Change scala-2.12 to scala-2.11 below.
#On Windows: Change bin/spark-submit to bin/spark-submit.cmd.
exec $SPARK/bin/spark-submit \
  --conf spark.driver.maxResultSize=2g \
  --conf $PARTITIONS \
  --conf $SPLIT \
  --master $MASTER \
  --class discount.spark.Discount target/scala-2.12/Discount-assembly-1.4.0.jar $*
