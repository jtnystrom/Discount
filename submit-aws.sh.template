#!/bin/bash
#Script to submit as an Amazon AWS EMR step. Copy this file to aws-discount.sh and 
#edit variables accordingly.

#For this script to work, it is necessary to install and configure the AWS CLI.

#The first argument is the cluster ID. The remaining arguments will be passed to the Discount driver process.
CLUSTER=$1
shift

#Bucket to store discount jars and data files
BUCKET=s3://my-bucket/discount

#Change 2.12 to 2.11 below if compiling for scala 2.11.

#Copy jars and data files the first time only, after which the following lines can safely be commented out
aws s3 cp target/scala-2.12/Discount-assembly-2.1.0.jar $BUCKET/

for f in PASHA/pasha_all*txt
do
  aws s3 cp $f $BUCKET/PASHA/
done

#Max size of input splits in bytes. A smaller number reduces memory usage but increases the number of 
#partitions for the first stage. If this variable is unset, Spark's default of 128 MB will be used.
SPLIT="spark.hadoop.mapreduce.input.fileinputformat.split.maxsize=$((64 * 1024 * 1024))"

#High memory
PARTITIONS=##spark.sql.shuffle.partitions=4000
#Low memory
#PARTITIONS=##spark.sql.shuffle.partitions=12000

COMMAND=( --conf $SPLIT \
  --conf $PARTITIONS \
 --class com.jnpersson.discount.spark.Discount $BUCKET/Discount-assembly-2.1.0.jar $*)

#Turn off paging for output
export AWS_PAGER=""

RUNNER_ARGS="spark-submit"
for PARAM in ${COMMAND[@]}
do
  RUNNER_ARGS="$RUNNER_ARGS,$PARAM"
done

aws emr add-steps --cluster $CLUSTER --steps Type=CUSTOM_JAR,Name=Discount,ActionOnFailure=CONTINUE,Jar=command-runner.jar,Args=\[$RUNNER_ARGS\]
